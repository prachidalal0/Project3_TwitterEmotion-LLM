# -*- coding: utf-8 -*-
"""Prachi_Dalal_HW6_Exp3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FyaC1AVVAx8cMaKAqUojV4124V2u3SZv

# <Font color = 'indianred'>**HW6 - Prachi Dalal**

# <Font color = 'indianred'>**Set Environment**
"""

# If in Colab, then import the drive module from google.colab
if 'google.colab' in str(get_ipython()):
  from google.colab import drive
  # Mount the Google Drive to access files stored there
  drive.mount('/content/drive')
    # Install the latest version of torchtext library quietly without showing output
  !pip install transformers evaluate wandb datasets accelerate -U -qq ## NEW LINES ##
  basepath = '/content/drive/MyDrive/BUAN 6342'
else:
  basepath = '/Users/prachidalal/Desktop/SPRING\ 2024/BUAN\ 6342'

# Importing PyTorch library for tensor computations and neural network modules
import torch
import torch.nn as nn

# For working with textual data vocabularies and for displaying model summaries
from torchtext.vocab import vocab

# General-purpose Python libraries for random number generation and numerical operations
import random
import numpy as np
import pandas as pd

# Utilities for efficient serialization/deserialization of Python objects and for element tallying
import joblib
from collections import Counter

# For creating lightweight attribute classes and for partial function application
from functools import partial

# For filesystem path handling, generating and displaying confusion matrices, and date-time manipulations
from pathlib import Path
from sklearn.metrics import confusion_matrix,hamming_loss, f1_score
from datetime import datetime

# For plotting and visualization
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

### NEW ##########################
# imports from Huggingface ecosystem
from transformers.modeling_outputs import SequenceClassifierOutput
from transformers import PreTrainedModel, PretrainedConfig
from transformers import TrainingArguments, Trainer
from datasets import Dataset
import evaluate

# wandb library
import wandb

#For Preprocessor
from sklearn.base import BaseEstimator, TransformerMixin
from bs4 import BeautifulSoup
import re
import spacy
import numpy as np
from nltk.stem.porter import PorterStemmer
import os

#ML Libraries
from sklearn.model_selection import train_test_split

# New libraries introduced in this notebook
from datasets import Dataset
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification, AutoModel
from transformers import AutoConfig
from transformers import PreTrainedModel, PretrainedConfig
from transformers import DataCollatorWithPadding
from transformers.modeling_outputs import SequenceClassifierOutput

# New libraries introduced in this notebook
import evaluate
from datasets import load_dataset, DatasetDict
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from transformers import AutoConfig
from transformers import pipeline
import wandb

get_ipython()

base_folder = Path(basepath)
model_folder = base_folder/'models'

model_folder.mkdir(exist_ok=True, parents = True)
model_folder

import torch
import transformers
import datasets
import sklearn
import spacy
import numpy
import joblib
import seaborn
import matplotlib
import wandb
import bs4

print("PyTorch version:", torch.__version__)
print("Transformers version:", transformers.__version__)
print("Datasets version:", datasets.__version__)
print("Scikit-learn version:", sklearn.__version__)
print("spaCy version:", spacy.__version__)
print("NumPy version:", numpy.__version__)
print("Joblib version:", joblib.__version__)
print("Seaborn version:", seaborn.__version__)
print("Matplotlib version:", matplotlib.__version__)
print("Weights & Biases version:", wandb.__version__)
print("Beautiful Soup version:", bs4.__version__)

pip install transformers datasets

pip install datasets

import pandas as pd
from datasets import Dataset
import re

# Read the CSV file into a pandas DataFrame
path = "/content/drive/MyDrive/BUAN 6342/train.csv"
df_train = pd.read_csv(path)

df_train.head()

class SpacyPreprocessor(BaseEstimator, TransformerMixin):

    def __init__(self, model, *, batch_size = 64, lemmatize=True, lower=True, remove_stop=True,
                remove_punct=True, remove_email=True, remove_url=True, remove_num=False, stemming = False,
                add_user_mention_prefix=True, remove_hashtag_prefix=False, basic_clean_only=False):

        self.model = model
        self.batch_size = batch_size
        self.remove_stop = remove_stop
        self.remove_punct = remove_punct
        self.remove_num = remove_num
        self.remove_url = remove_url
        self.remove_email = remove_email
        self.lower = lower
        self.add_user_mention_prefix = add_user_mention_prefix
        self.remove_hashtag_prefix = remove_hashtag_prefix
        self.basic_clean_only = basic_clean_only

        if lemmatize and stemming:
            raise ValueError("Only one of 'lemmatize' and 'stemming' can be True.")

        # Validate basic_clean_only option
        if self.basic_clean_only and (lemmatize or lower or remove_stop or remove_punct or remove_num or stemming or
                                      add_user_mention_prefix or remove_hashtag_prefix):
            raise ValueError("If 'basic_clean_only' is set to True, other processing options must be set to False.")

        # Assign lemmatize and stemming

        self.lemmatize = lemmatize
        self.stemming = stemming

    def basic_clean(self, text):
        soup = BeautifulSoup(text, "html.parser")
        text = soup.get_text()
        text = re.sub(r'[\n\r]', ' ', text)
        return text.strip()

    def spacy_preprocessor(self, texts):
        final_result = []
        nlp = spacy.load(self.model)

        # Disable unnecessary pipelines in spaCy model
        if self.lemmatize:
            # Disable parser and named entity recognition
            disabled_pipes = ['parser', 'ner']
        else:
            # Disable tagger, parser, attribute ruler, lemmatizer and named entity recognition
            disabled_pipes = ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']

        with nlp.select_pipes(disable=disabled_pipes):
          # Modify tokenizer behavior based on user_mention_prefix and hashtag_prefix settings
          if self.add_user_mention_prefix or self.remove_hashtag_prefix:
              prefixes = list(nlp.Defaults.prefixes)
              if self.add_user_mention_prefix:
                  prefixes += ['@']  # Treat '@' as a separate token
              if self.remove_hashtag_prefix:
                  prefixes.remove(r'#')  # Don't separate '#' from the following text
              prefix_regex = spacy.util.compile_prefix_regex(prefixes)
              nlp.tokenizer.prefix_search = prefix_regex.search

          # Process text data in parallel using spaCy's nlp.pipe()
          for doc in nlp.pipe(texts, batch_size=self.batch_size):
              filtered_tokens = []
              for token in doc:
                  # Check if token should be removed based on specified filters
                  if self.remove_stop and token.is_stop:
                      continue
                  if self.remove_punct and token.is_punct:
                      continue
                  if self.remove_num and token.like_num:
                      continue
                  if self.remove_url and token.like_url:
                      continue
                  if self.remove_email and token.like_email:
                      continue

                  # Append the token's text, lemma, or stemmed form to the filtered_tokens list
                  if self.lemmatize:
                      filtered_tokens.append(token.lemma_)
                  elif self.stemming:
                      filtered_tokens.append(PorterStemmer().stem(token.text))
                  else:
                      filtered_tokens.append(token.text)

              # Join the tokens and apply lowercasing if specified
              text = ' '.join(filtered_tokens)
              if self.lower:
                  text = text.lower()
              final_result.append(text.strip())

        return final_result


    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        try:
            if not isinstance(X, (list, np.ndarray)):
                raise TypeError(f'Expected list or numpy array, got {type(X)}')

            x_clean = [self.basic_clean(text).encode('utf-8', 'ignore').decode() for text in X]

            # Check if only basic cleaning is required
            if self.basic_clean_only:
                return x_clean  # Return the list of basic-cleaned texts

            x_clean_final = self.spacy_preprocessor(x_clean)
            return x_clean_final

        except Exception as error:
            print(f'An exception occurred: {repr(error)}')

# Create an instance of the SpacyPreprocessor class
preprocessor = SpacyPreprocessor(model='en_core_web_sm', lemmatize=True, lower=True, remove_stop=True,
                                        remove_punct=True, remove_email=True, remove_url=True, remove_num=False,
                                        stemming=False, add_user_mention_prefix=True, remove_hashtag_prefix=False,
                                        basic_clean_only=False)

# Transform the text data using the SpacyPreprocessor
cleaned_tweets = preprocessor.fit_transform(df_train['Tweet'].values)

df_train['cleaned_tweets']= cleaned_tweets
df_train.head()

from sklearn.model_selection import train_test_split

# Split the DataFrame into features (X) and labels (y)
X = df_train['cleaned_tweets'].values.tolist()
y = df_train[['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']].values.tolist()

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

y_train_float = torch.tensor(y_train, dtype=torch.float32)
y_val_float = torch.tensor(y_val, dtype=torch.float32)

train_split = Dataset.from_dict({
    'texts': X_train,
    'labels': y_train_float
})

val_split = Dataset.from_dict({
    'texts': X_val,
    'labels': y_val_float
})

train_val = DatasetDict(
    {"train": train_split, "valid": val_split})

train_val

checkpoint = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
def tokenize_fn(batch):
    return tokenizer(batch["texts"], truncation=True, padding=True)

from transformers import AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=11, problem_type="multi_label_classification")

tokenized_dataset= train_val.map(tokenize_fn, batched=True)
tokenized_dataset = tokenized_dataset.remove_columns(
    ['texts']
)
# Set the format for the tokenized dataset
tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

# Initialize data collator with padding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

tokenized_dataset

tokenized_dataset['train'][0]

model

# Update configuration for DistilRoBERTa
config = AutoConfig.from_pretrained(checkpoint)
num_labels = 11  # Number of classes
config.num_labels = num_labels

from sklearn.metrics import hamming_loss,f1_score

def compute_metrics(eval_pred):

    predictions = eval_pred.predictions[0]
    labels = eval_pred.label_ids
    labels = labels.astype('int32')
    predictions = (predictions>=0).astype('int32')

    hamming_loss_value = hamming_loss(labels, predictions)

    #use weighted to account for class imbalance
    f1_score_macro = f1_score(labels, predictions, average='macro')
    evaluations = {
        'hamming_loss' : hamming_loss_value,
        'f1_score': f1_score_macro,
    }
    return evaluations

# Configure training arguments
training_args = TrainingArguments(
    output_dir=str(model_folder),
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    weight_decay=0.01,
    learning_rate=5e-5,
    num_train_epochs=3,
    evaluation_strategy='steps',
    eval_steps=25,
    save_strategy="steps",
    save_steps=25,
    load_best_model_at_end=True,
    save_total_limit=2,
    metric_for_best_model="f1_score",
    greater_is_better=True,
    logging_strategy='steps',
    logging_steps=100,
    report_to='wandb',
    run_name='tweet_hf_trainer',
    fp16=True,
)

# Initialize trainer with the updated datasets
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset['train'],
    eval_dataset=tokenized_dataset['valid'],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
)

!wandb login

# Commented out IPython magic to ensure Python compatibility.
wandb.init(project="sentiment-analysis-hf-trainer-Flan-T5-base")
# %env WANDB_PROJECT=sentiment-analysis-hf-trainer-Flan-T5-base
wandb.init()

from accelerate import Accelerator
accelerator = Accelerator()

labels_shape = tokenized_dataset['train']['labels'].shape
print("Shape of labels:", labels_shape)

trainer.train()

trainer.evaluate()

valid_output = trainer.evaluate(tokenized_dataset["valid"])

valid_output = trainer.predict(tokenized_dataset["valid"])
valid_preds = (valid_output.predictions[0] > 0).astype('int32')
valid_labels = valid_output.label_ids

from sklearn.metrics import multilabel_confusion_matrix
def plot_confusion_matrix(valid_labels, valid_preds, class_labels):
    """
    Plots a confusion matrix for each individual class.

    Args:
        valid_labels (array-like): True labels of the validation data.
        valid_preds (array-like): Predicted labels of the validation data.
        class_labels (list): List of class names for the labels.
    """
    # Compute the multilabel confusion matrix
    mcm = multilabel_confusion_matrix(valid_labels, valid_preds)

    # Plot confusion matrices for each individual class
    num_classes = len(class_labels)
    fig, axes = plt.subplots(num_classes, 1, figsize=(8, num_classes * 4), squeeze=False)

    for i, (cm, label) in enumerate(zip(mcm, class_labels)):
        ax = axes[i, 0]
        sns.heatmap(cm, annot=True, fmt=".0f", cmap="Blues", ax=ax)
        ax.set_title(f'Confusion Matrix for Class: {label}')
        ax.set_xlabel('Predicted Labels')
        ax.set_ylabel('True Labels')
        ax.set_xticklabels(['False', 'True'])
        ax.set_yticklabels(['False', 'True'])
        ax.set_aspect('equal')

    plt.tight_layout()
    plt.show()

plot_confusion_matrix(valid_labels, valid_preds, class_labels=list(config.id2label.values()))

print("Validation Macro F1 Score: ", f1_score(valid_labels,valid_preds, average= 'macro'), "\nValidation Hamming Loss: ",hamming_loss(valid_labels,valid_preds))

best_model_checkpoint_step = trainer.state.best_model_checkpoint.split('-')[-1]
print(f"The best model was saved at step {best_model_checkpoint_step}.")

# Read the CSV file into a pandas DataFrame
path = "/content/drive/MyDrive/BUAN 6342/test.csv"
df_test = pd.read_csv(path)

df_test.head()

# Replace "NONE" labels with 0s
label_columns = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']
df_test[label_columns] = df_test[label_columns].replace('NONE', 0)

# Verify the changes
df_test.head()

cleaned_tweets_test = preprocessor.fit_transform(df_test['Tweet'].values)

df_test['cleaned_tweets_test']= cleaned_tweets_test
df_test.head()
X_test = df_test['cleaned_tweets_test'].values.tolist()
y_test = df_test[['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']].values.tolist()
y_test_float = torch.tensor(y_test, dtype=torch.float32)
test_split = Dataset.from_dict({
    'texts': X_test,
    'labels': y_test_float
})

test_split

test_set_tokenized = test_split.map(tokenize_fn, batched=True)

training_args = TrainingArguments(
    output_dir=str(model_folder),
    per_device_eval_batch_size=16,
    do_train=False,
    do_eval=True,
    metric_for_best_model="f1_score",
    greater_is_better=True,
    logging_strategy='epoch',  # Log metrics at the end of each epoch
    report_to='wandb',
    run_name='tweet_hf_trainer',
    fp16=True,
)

test_trainer = Trainer(
    model=model,
    args=training_args,
    eval_dataset=test_set_tokenized,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
)

test_results = test_trainer.evaluate()

test_predictions = trainer.predict(test_set_tokenized)

test_predictions.metrics

test_predictions.label_ids

test_preds = (valid_output.predictions[0] > 0).astype('int32')
test_labels = valid_output.label_ids

plot_confusion_matrix(test_labels, test_preds, class_labels=list(config.id2label.values()))

print("Test Macro F1 Score: ", f1_score(test_labels,test_preds, average= 'macro'), "\nTest Hamming Loss: ",hamming_loss(test_labels,test_preds))

