# -*- coding: utf-8 -*-
"""3_Understanding_dropout_batch_norm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14IGxI0KnQ46JqwrGo7ThfjzjiZUWOUYK

# <font color = 'indianred'> **Import Libraries**
"""

import torch
import torch.nn as nn
import numpy as np
from sklearn.model_selection import train_test_split

"""# <font color = 'indianred'>**Understanding Dropout, Batchnorm1d, model.state_dict**

## <font color = 'indianred'>**Dropout**

<img src ="https://drive.google.com/uc?export=view&id=1f7KmsmF1TXZFUNOJpWBH2P4WkawnVH3Z" width =500>
"""

torch.manual_seed(42)
inp = torch.tensor([1.0, 2.0, 3, 4, 5])
print(inp)
model = nn.Dropout(p=0.3)
output = model(inp)
print(output)

1/0.7

inp * (1/0.7)

inp.sum(), output.sum()

"""### <font color = 'indianred'>**Dropout with model.train()**"""

torch.manual_seed(42)
inp = torch.tensor([1.0, 2.0, 3, 4, 5])
print(inp)
model = nn.Dropout(p=0.5)
model.train()
output = model(inp)
print(output)

"""### <font color = 'indianred'>**Dropout with model.eval()**"""

# model.eval() ignores dropout and batch normalization layers

inp = torch.tensor([1.0, 2.0, 3, 4, 5])
print(inp)
model = nn.Dropout(p=0.4)
model.eval()
output = model(inp)
print(output)

"""## <font color = 'indianred'>**Model.eval vs torch.no_grad()**"""

N = 10
# random data on the x-axis in (-5, +5)
X = np.random.random((N, 2))*10-5

# a line plus some noise
Y = 0.5*X[:, 0] + 0.2*X[:, 1]-1 + np.random.randn(N)

X_train, X_test, y_train, y_test = train_test_split(
    X, Y, test_size=0.33, random_state=41)

X_train = torch.from_numpy(X_train.astype(np.float32))
X_test = torch.from_numpy(X_test.astype(np.float32))
y_train = torch.from_numpy(y_train.astype(np.float32).reshape(-1, 1))
y_test = torch.from_numpy(y_test.astype(np.float32).reshape(-1, 1))

model = nn.Sequential(nn.Dropout(p=0.4),
                      nn.Linear(2, 1)
                      )
criterion = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.05)

n_epochs = 1
train_losses = np.zeros(n_epochs)
test_losses = np.zeros(n_epochs)

for i in range(n_epochs):
    # zero the parameter gradients
    optimizer.zero_grad()

    model.train()
    # Forward pass
    out_train = model(X_train)  # we are using both the layers together here
    # We are using the first layer only here
    out_train_drop = model[0](X_train)
    loss_train = criterion(out_train, y_train)

    # Backward and optimize
    loss_train.backward()
    optimizer.step()

    model.eval()
    # test loss and prediction
    with torch.no_grad():
        out_test = model(X_test)
        out_test_drop = model[0](X_test)
        loss_test = criterion(out_test, y_test)

    # Save losses
    train_losses[i] = loss_train.item()
    test_losses[i] = loss_test.item()

print(out_train.requires_grad)
print(loss_train.requires_grad)
print(out_test.requires_grad)
print(loss_test.requires_grad)

print('\nOutput after Dropout in Train', out_train_drop)
print('\nX_train', X_train)
print('\nOutput after Dropout in Test', out_test_drop)
print('\nX_test', X_test)

model.state_dict()

list(model.parameters())

"""## <font color = 'indianred'>**Batchnorm1d**

<img src ="https://drive.google.com/uc?export=view&id=1f6TJdYfRJdQ10GVO6Q2ZX7biejwwykkq" width =300>
"""

X = torch.randn(3, 2) * 5 + 10

B = nn.BatchNorm1d(2, affine=False)
y = B(X)

mu = torch.mean(X, axis=0)
var_ = torch.var(X, axis=0, unbiased=False)
sigma = torch.sqrt(var_ + 1e-5)
z = (X - mu)/sigma

# the ratio below should be equal to one
print(z / y)

"""### <font color = 'indianred'>**Batchnorm with model.train() and model.eval()**
- During training, this layer keeps a running estimate of its computed mean and variance. The running sum is kept with a default momentum of 0.1.

- During evaluation, this running mean/variance is used for normalization.
"""

torch.manual_seed(0)
X1 = torch.randn(3, 2) * 5 + 10
print('X1', X1, end='\n\n', sep='\n')
model = nn.Sequential()
model.add_module('batchnorm', nn.BatchNorm1d(2, momentum=0.1))
print('X1.mean()', X1.mean(axis=0), end='\n\n', sep='\n')
y = model(X1)
print(y)
print('Running Mean', model[0].running_mean, end='\n\n', sep='\n')

0.1 * torch.tensor([7.1295, 8.1273]) + 0.9 * torch.tensor([0.7129, 0.8127])

for i in range(3):
    model.train()
    y1 = model(X1)
    # if (i % 10) ==0:
    print(model[0].running_mean)

for i in range(100):
    model.train()
    y1 = model(X1)
    if (i % 10) == 0:
        print(model[0].running_mean)

for i in range(100):
    model.eval()
    y1 = model(X1)
    if (i % 10) == 0:
        print(model[0].running_mean)

model.state_dict()

list(model.parameters())
