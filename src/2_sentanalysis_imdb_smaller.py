# -*- coding: utf-8 -*-
"""2_SentAnalysis_IMDB_smaller.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-0Y8m1LpVtLsqFGS-lH11d4BdjzQ8EIR

<h1 align='center'><u><font color ='pickle'>Sentiment Analysis on IMDB dataset</u></h1></font>

# <font color ='pickle'>**Notebook Overview**</font>
* Till now we have learned Data processing, Featurization such as CountVectorizer, TFIDFVectorizer, and also Feature Engineering.
* So now lets use all those technique and create a classification pipeline for it.
* We will create 3 pipelines:
    1. Data Preprocessing + Sparse Embeddings (TF-IDF) + ML Model pipeline
    2. Feature Engineering + ML Model pipeline
    3. Feature Engineering + Data Preprocessing + Sparse Embeddings(TF-IDF) + ML Model pipeline.

# <font color ='pickle'>**Installing/Importing libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload 2

# Import necessary libraries
import pandas as pd
from pathlib import Path
import sys

# Import the joblib library for saving and loading models
import joblib

# Import scikit-learn classes for building models
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.base import TransformerMixin, BaseEstimator

# Import the scipy library for working with sparse matrices
from scipy.sparse import csr_matrix

if 'google.colab' in str(get_ipython()):
    from google.colab import drive
    drive.mount('/content/drive')

    !pip install -U nltk -qq
    !pip install -U spacy -qq
    !python -m spacy download en_core_web_sm -qq

    basepath = '/content/drive/MyDrive/data'
    sys.path.append('/content/drive/MyDrive/data/custom-functions')
else: #have to use custom files
    basepath = '/home/harpreet/Insync/google_drive_shaannoor/data'
    sys.path.append(
        '/home/harpreet/Insync/google_drive_shaannoor/data/custom-functions')

sys.path

#created by professor
import SpacyPreprocessor as cp
from FeaturizerSpacy import ManualFeatures
from plot_learning_curve import plot_learning_curve

#may take a long time to run so we want to make sure we can save the model so we dont have to reload them everytime
base_folder = Path(basepath)
data_folder = base_folder/'datasets/aclImdb'
model_folder = base_folder/'models/nlp_spring_2024/imdb'

"""- `Path(basepath)` converts the string `basepath` into a `Path` object, which makes path manipulations (like joining paths) more intuitive and cross-platform (works on Windows, macOS, Linux without needing to worry about forward vs backward slashes).
- The resulting `Path` object is stored in the variable `base_folder`.

- `data_folder = base_folder/'datasets/aclImdb'`creates a path to the dataset folder.
    - `base_folder` is the `Path` object created in the previous line.
    - `/'datasets/aclImdb'` utilizes the `/` operator of the `Path` object to append the `'datasets/aclImdb'` string to `base_folder`. This results in a new `Path` object that points to the `aclImdb` directory inside a `datasets` directory, which is assumed to be in the base folder.
    - The complete path is stored in the variable `data_folder`.

- `model_folder = base_folder/'models/nlp_spring_2024/imdb'`
    - Similar to the previous line, this line is constructing a path, but this time for the model folder.
"""

#parent folder? creates parent folder for you
model_folder.mkdir(exist_ok=True, parents=True)

"""- `mkdir` is a method available on `Path` objects in the `pathlib` library. It's used to create a new directory at the path represented by the `Path` object (`model_folder` in this case).
   - If the directory already exists, `mkdir` would typically throw a `FileExistsError`, unless instructed otherwise by the parameters.
   - If **`exist_ok` is set to `True`**, it allows the `mkdir` method to complete successfully without throwing an error even if the directory already exists. This can be useful for running scripts multiple times or ensuring that a directory is in place without needing to check its existence beforehand.
   - Normally, `mkdir` would throw an error if the parent directory of the directory you are trying to make doesn't exist. For instance, if you're trying to create `/a/b/c` and `/a/b` doesn't exist, you'd get an error.
   - When `parents` is set to `True`, it instructs `mkdir` to create all the missing parent directories needed to fulfill the requested path. So in the example above, it would create `/a/b` and then `/a/b/c`.

# <font color ='pickle'>**Load dataset**

For this notebook, we will use IMDB movie review dataset. <br>
LInk for complete dataset: http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz.

We downloaded the dataset in the previous lecture.
We will now use the saved train.csv and test.csv file.
"""

# location of train and test files
train_file = data_folder / 'train.csv'
test_file = data_folder / 'test.csv'

# creating Pandas Dataframe
train_data = pd.read_csv(train_file, index_col=0)
test_data = pd.read_csv(test_file, index_col=0)

# print shape of the datasets
print(f'Shape of Training data set is : {train_data.shape}')
print(f'Shape of Test data set is : {test_data.shape}')

# Printing top 5 train records
train_data.head()

# Printing top 5 test records
test_data.head()

"""# <font color ='pickle'>**Create Subset of Data**"""

train_smaller = train_data.sample(frac=0.1, replace=True, random_state=1)
#10% of dataset, take small subset for initial experiment then practice on whole dataset

test_smaller = test_data.sample(frac=0.1, replace=True, random_state=1)

"""# <font color ='pickle'>**Exploratory Data Analysis**"""

# Printing basic info
train_smaller.info()

# Checking distribution of class labels for train dataset
train_smaller['Labels'].value_counts()
#always look at distribution of outcome to prevent bias  --> if not then there is a imbalance in the dataset
#best way is cost sensitive learning, give more weightage for error with the minority class, then it won't have that bias?
#in this example, the imbalance is not an issue

# Checking distribution of class labels for test dataset
test_smaller['Labels'].value_counts()

"""As we can see our data is well balannced.

# <font color ='pickle'>**Classification Pipeline**

## <font color ='pickle'>**PreProcessing**
"""

X_train = train_smaller['Reviews'].values
X_test = test_smaller['Reviews'].values
y_train = train_smaller['Labels'].values
y_test = test_smaller['Labels'].values

print(f'X_train: {X_train.shape} y_train: {y_train.shape}')
print(f'X_test: {X_test.shape} y_test: {y_test.shape}')

cp.SpacyPreprocessor??

#saving this in a file
X_train_cleaned_bow = cp.SpacyPreprocessor(model='en_core_web_sm', batch_size = 500).transform(X_train)

# save  this to a file
# save array of cleaned dataset into a file that you can load lateron
# do not make this part part of the pipeline
# just cleaned separately and put them into files we can use lateron
file_X_train_cleaned_bow = data_folder / 'x_train_cleaned_bow_small.pkl'
joblib.dump(X_train_cleaned_bow, file_X_train_cleaned_bow)

#no data leakage in preprocessing, so don't make it part of the pipeline

X_test_cleaned_bow = cp.SpacyPreprocessor(model='en_core_web_sm', batch_size = 500).transform(X_test)

# save  this to a file
file_X_test_cleaned_bow = data_folder / 'x_test_cleaned_bow_small.pkl'
joblib.dump(X_test_cleaned_bow, file_X_test_cleaned_bow)

"""## <font color ='pickle'>**Pipeline 1: Data Preprocessing + Sparse Embeddings (TF-IDF) + ML Model**

### <font color ='pickle'>**Create Pipeline**
"""

classifier_1 = Pipeline([
    ('vectorizer', TfidfVectorizer(analyzer='word', token_pattern=r"[\S]+")),
    ('classifier', LogisticRegression(max_iter=10000)),])
# c in logistic regression is the inverse of lamba, increasing lamba the regularization will increase
# dont do any regaulization at first --> high number of c, overfits meaning we decrease c later on, if it underfits
# trying 9 different combinations in cross validation

"""<font color ='pickle'>*Question:
Why preprocessor is not part of the Pipleine but vectorizer is part of the pipeline?*

### <font color ='pickle'>**Parameter Grid**
"""

param_grid_classifier_1 = {'vectorizer__max_features': [1000, 2000, 5000],
                           'classifier__C': [1, 10, 100,]
                           }

"""### <font color ='pickle'>**Specify GridSearch**"""

# We will now use Gridserach to find fine tune hyperparameters using cross validation
# The advantage of using pipelines is that we can avoid data leakage
# Since we have balanced data set, we will use default scoring method of accuracy
# the typical value of cv used is 5. We are using 3, just for demonstration.

grid_classifier_1 = GridSearchCV(
    estimator=classifier_1, param_grid=param_grid_classifier_1, cv=3) #9*3 = 27, dont make preprocessing a part of pipeline bc it will clean 27 times
    #doing it 28 times because retraining at the end



"""### <font color ='pickle'>**Fit the model**"""

# Fit the model on training data
X_train_cleaned_bow= joblib.load(file_X_train_cleaned_bow)
grid_classifier_1.fit(X_train_cleaned_bow, y_train)

"""### <font color = 'pickle'>**Get Best Params**"""

print(f'Best cross-validation score: {grid_classifier_1.best_score_:.2f}')
print("\nBest parameters: ", grid_classifier_1.best_params_)
print("\nBest estimator: ", grid_classifier_1.best_estimator_)
#instead of "retrained model" in the chart it is supposed to be splitting into training & test data

"""<img src ="https://drive.google.com/uc?export=view&id=1iK80BvXepRL1xHwJWqQa14BiNhJMVH9S" width =600 >

* In the above figure we are splitting the dataset in to training and test set.
* We used cross-validation to find the best parameters.
* We use the best parameters and the training set to build a model with the best parameters, and finally evaluate it on the test set.
* GridSearch CV, does all the steps (implemented using for loop in the code above). Let's discuss more about this .

Figure inspired from https://scikit-learn.org/stable/modules/cross_validation.html

### <font color ='pickle'>**Save Model**
"""

file_best_estimator_pipeline1_round1 = model_folder / \
    'pipeline1_round1_best_estimator.pkl'
file_complete_grid_pipeline1_round1 = model_folder / \
    'pipeline1_round1_complete_grid.pkl'

joblib.dump(grid_classifier_1.best_estimator_,
            file_best_estimator_pipeline1_round1)
joblib.dump(grid_classifier_1, file_complete_grid_pipeline1_round1)

"""### <font color ='pickle'>**Load Saved Model**"""

# load the saved model
best_estimator_pipeline1_round1 = joblib.load(
    file_best_estimator_pipeline1_round1)
complete_grid_pipeline1_round1 = joblib.load(
    file_complete_grid_pipeline1_round1)

"""### <font color ='pickle'>**Plot Learning Curve**"""

# plot learning curces
plot_learning_curve(best_estimator_pipeline1_round1, 'Learning Curves',
                    X_train_cleaned_bow, y_train, n_jobs=-1)
#learning curve tells us overfitting / underfitting
#training is high --> overfitting --> only using 10% of the data so we can try adding more data and if it still overfits we can add regularization

"""<font color ='indianred'>**Observations**</font>
<br>
Clearly there is <font color ='indianred'>**overfitting**</font>. In case of overfitting we can improve results by

1. Adding more data (training model on complete dataset)
2. By hyperparameter tuning (reduce model complexity) of logistic regression and vectorizer.

### <font color = 'pickle'>**Check Cross Validation Score and Train Score**
"""

# let's check the train scores
print(best_estimator_pipeline1_round1.score(
    X_train_cleaned_bow, y_train))

# let's check the cross validation score
print(complete_grid_pipeline1_round1.best_score_)

"""### <font color ='pickle'>**Evaluate model on test datset**"""

# Final Pipeline
def final_pipeline(text):
    # cleaned_text = cp.SpacyPreprocessor(model='en_core_web_sm').transform(text)
    cleaned_text = joblib.load(file_X_test_cleaned_bow)
    best_estimator_pipeline1_round1 = joblib.load(
        file_best_estimator_pipeline1_round1)
    predictions = best_estimator_pipeline1_round1.predict(cleaned_text)
    return predictions

# predicted values for Test data set
y_test_pred = final_pipeline(X_test)

"""### <font color ='pickle'>**Classification report for test dataset**

* As we have already seen, our dataset is well balanced. Hence we have used accuracy as our scoring method.
* However, it is a good idea to also look at classification report which gives precision, recall, and f1-score for both labels and average score also.
"""

print('\nTest set classification report:\n\n',
      classification_report(y_test, y_test_pred))

"""## <font color ='pickle'>**Pipeline 2: Data Preprocessing + Manual Features + ML Model pipeline**

In this case we will extract following features and use these as the input to our logistic regression.
  1. count of words
  2. count of characters
  3. count of characters without space
  4. average word length
  5. count of numbers
  7. number of nouns or propernouns
  8. number of aux
  9. number of verbs
  10. number of adjectives
  11. number of ner (entiites)

### <font color ='pickle'>**Generate Manual Features**
"""

cp.SpacyPreprocessor??

#doing basic cleaning, then adding our manual featurizor
#manual featurizor there is no data leakage (fit function is empty, not storing for transformation)
X_train_cleaned_basic = cp.SpacyPreprocessor(model='en_core_web_sm',
                                                    lemmatize=False, lower=False,
                                                    remove_stop=False, remove_punct=False,
                                                    remove_email=False, remove_url=False,
                                                     add_user_mention_prefix=False,
                                                     basic_clean_only=True).transform(X_train)

# save  this to a file
file_X_train_cleaned_basic = data_folder / 'x_train_cleaned_basic_small.pkl'
joblib.dump(X_train_cleaned_basic, file_X_train_cleaned_basic)

X_test_cleaned_basic = cp.SpacyPreprocessor(model='en_core_web_sm',
                                                    lemmatize=False, lower=False,
                                                    remove_stop=False, remove_punct=False,
                                                    remove_email=False, remove_url=False,
                                                     add_user_mention_prefix=False,
                                                     basic_clean_only=True).transform(X_test)

# save  this to a file
file_X_test_cleaned_basic = data_folder / 'x_test_cleaned_basic_small.pkl'
joblib.dump(X_test_cleaned_basic, file_X_test_cleaned_basic)

ManualFeatures??

featurizer = ManualFeatures(spacy_model='en_core_web_sm', batch_size = 1000)

X_train_cleaned_basic = joblib.load(file_X_train_cleaned_basic)

X_train_features, feature_names = featurizer.fit_transform(X_train_cleaned_basic)

X_train_features[0:3]

feature_names

"""### <font color ='pickle'>**Create Pipeline**"""

classifier_2 = Pipeline([
    ('classifier', LogisticRegression(max_iter=10000)),
])

"""### <font color ='pickle'>**Parameter Grid**"""

# we will try to overfit a samll dataset first

param_grid_classifier_2 = {'classifier__C': [100000]}

"""### <font color ='pickle'>**Specify GridSearch**"""

# the typical value of cv used is 5. We are using 3, just for demonstration.

grid_classifier_2 = GridSearchCV(estimator=classifier_2,
                                 param_grid=param_grid_classifier_2,
                                 cv=3)

"""### <font color ='pickle'>**Fit the Model**"""

# Fit the model on training data
grid_classifier_2.fit(X_train_features, y_train)

"""### <font color ='pickle'>**Get Best Params**"""

print(f'Best cross-validation score: {grid_classifier_2.best_score_:.2f}')
print("\nBest parameters: ", grid_classifier_2.best_params_)
print("\nBest estimator: ", grid_classifier_2.best_estimator_)

"""### <font color ='pickle'>**Save Model**"""

file_best_estimator_pipeline2_round1 = model_folder / \
    'pipeline2_round1_best_estimator.pkl'
file_complete_grid_pipeline2_round1 = model_folder / \
    'pipeline2_round1_complete_grid.pkl'

joblib.dump(grid_classifier_2.best_estimator_,
            file_best_estimator_pipeline2_round1)
joblib.dump(grid_classifier_2, file_complete_grid_pipeline2_round1)

"""### <font color ='pickle'>**Load Saved Model**"""

# load the saved model
best_estimator_pipeline2_round1 = joblib.load(
    file_best_estimator_pipeline2_round1)
complete_grid_pipeline2_round1 = joblib.load(
    file_complete_grid_pipeline2_round1)

"""### <font color ='pickle'>**Plot Learning Curve**"""

# plot learning curces
plot_learning_curve(best_estimator_pipeline2_round1, 'Learning Curves',
                    X_train_features, y_train, n_jobs=-1)
#regularization might make it worse, you have to complicate the model, adding polynomial features to logistic regression

"""<font color ='indianred'>**Observations** </font>
<br>
Clearly there is <font color ='indianred'> **underfitting** </font>. In case of underfitting we can improve results by

1. By hyperparameter tuning (increase model complexity) of logistic regression.
2. Add New features or Polynomial terms

### <font color = 'pickle'>**Check Cross Validation Score and Train Score**
"""

# let's check the train scores
print(best_estimator_pipeline2_round1.score(X_train_features, y_train))

# let's check the cross validation score
print(complete_grid_pipeline2_round1.best_score_)

"""### <font color ='pickle'>**Evaluate model on test datset**"""

# Final Pipeline
def final_pipeline(text):
    text_cleaned = joblib.load(file_X_test_cleaned_basic)
    features, feature_names = featurizer.fit_transform(text_cleaned)
    best_estimator_pipeline2_round1 = joblib.load(
        file_best_estimator_pipeline2_round1)
    predictions = best_estimator_pipeline2_round1.predict(features)
    return predictions

# predicted values for Test data set
y_test_pred = final_pipeline(X_test)

"""### <font color ='pickle'>**Classification report for test dataset**"""

print('\nTest set classification report:\n\n',
      classification_report(y_test, y_test_pred))

"""## <font color ='pickle'>**Pipeline 3: Combine Manual Features and TF-IDF vectors**

### <font  color ='pickle'>**Combine Manual Features and tfidf features**

<font size = 4, color ='indianred'>**The approach described in this section is generic and can be used to combine any text with non-text related variables**.
"""

X_train_cleaned_bow = joblib.load(file_X_train_cleaned_bow)

X_train_final = pd.concat((pd.DataFrame(X_train_cleaned_bow, columns=['cleaned_text']),
                           pd.DataFrame(X_train_features, columns=feature_names)), axis=1)

X_train_final.head()
#not part of pipeline
#have text & numerical data

X_train_final.info()

subset = X_train_final[0:10]

subset.shape

class SparseTransformer(TransformerMixin, BaseEstimator):
    def __init__(self):
        pass

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        return csr_matrix(X)
#no transformer available in skit-learn, have to make out own
#pass numeric through sparse transformer & number through count

sparse_features = Pipeline([('sparse', SparseTransformer()), ])
vectorizer = Pipeline([('tfidf', TfidfVectorizer(max_features=5)), ])

sparse_features.fit_transform(subset.iloc[:, 1:])

vectorizer.fit_transform(subset.iloc[:, 0])

combined_features = ColumnTransformer(
    transformers=[
        ('tfidf', vectorizer, 'cleaned_text'),
    ], remainder=sparse_features
)
#applies vectorization to text
#applied sparse to non text?
#column transformer
#both goes through logistic regression

test = combined_features.fit_transform(subset)

test

test.dtype

"""### <font color ='pickle'>**Create Final Pipeline**"""

classifier_3 = Pipeline([('combined_features',  combined_features),
                         ('classifier', LogisticRegression(max_iter=10000)),
                         ])

classifier_3.get_params().keys()

"""### <font color ='pickle'>**Parameter Grid**"""

# We are exploring a small combination of parameters
# If the search space is very large then we should use RandomSerachCV or some other methods

param_grid_classifier_3 = {'combined_features__tfidf__tfidf__max_features': [500, 1000, 2000],
                           'classifier__C': [0.1, 1, 10]
                           }

"""### <font color ='pickle'>**Specify GridSearch**"""

# We will now use Gridserach to find fine tune hyperparameters using cross validation
# the typical value of cv used is 5. We are using 3, just for demonstration.

grid_classifier_3 = GridSearchCV(estimator=classifier_3,
                                 param_grid=param_grid_classifier_3,
                                 cv=3)

"""### <font color ='pickle'>**Fit the Model**"""

# Fit the model on training data
grid_classifier_3.fit(X_train_final, y_train)

"""### <font color ='pickle'>**Get Best Params**"""

print(
    "Best cross-validation score: {:.2f}".format(grid_classifier_3.best_score_))
print("\nBest parameters: ", grid_classifier_3.best_params_)
print("\nBest estimator: ", grid_classifier_3.best_estimator_)

"""### <font color ='pickle'>**Save Model**"""

file_best_estimator_pipeline3_round1 = model_folder / \
    'pipeline3_round1_best_estimator.pkl'
file_complete_grid_pipeline3_round1 = model_folder / \
    'pipeline3_round1_complete_grid.pkl'

joblib.dump(grid_classifier_3.best_estimator_,
            file_best_estimator_pipeline3_round1)
joblib.dump(grid_classifier_3, file_complete_grid_pipeline3_round1)

"""### <font color ='pickle'>**Load Saved Model**"""

# load the saved model
best_estimator_pipeline3_round1 = joblib.load(
    file_best_estimator_pipeline3_round1)
complete_grid_pipeline3_round1 = joblib.load(
    file_complete_grid_pipeline3_round1)

"""### <font color ='pickle'>**Plot Learning Curve**"""

# plot learning curces
plot_learning_curve(best_estimator_pipeline3_round1, 'Learning Curves',
                    X_train_final, y_train, n_jobs=-1)
#CV score of pipeline 3 is slightly lower, then doing hyperparameter tuning?

"""<font color ='indianred'>**Observations**
<br></font>
Clearly there is <font color ='indianred'>**overfitting**</font>. In case of overfitting we can improve results by

1. Adding more data (training model on complete dataset)
2. By hyperparameter tuning (reduce model complexity) of logistic regression and vectorizer.

### <font color = 'pickle'>**Check Cross Validation Score and Train Score**
"""

# let's check the train scores
print(best_estimator_pipeline3_round1.score(X_train_final, y_train))

# let's check the cross validation score
print(complete_grid_pipeline3_round1.best_score_)

"""### <font color ='pickle'>**Evaluate model on test datset**"""

X_test.shape

# Final Pipeline
def final_pipeline(text):
    # cleaned_text = cp.SpacyPreprocessor(model='en_core_web_sm', batch_size = 1000).transform(text)
    cleaned_text = joblib.load(file_X_test_cleaned_bow)
    X_features, feature_names = featurizer.fit_transform(text)
    X_final = pd.concat((pd.DataFrame(cleaned_text, columns=['cleaned_text']),
                         pd.DataFrame(X_features, columns=feature_names)), axis=1)
    best_estimator_pipeline3_round1 = joblib.load(
        file_best_estimator_pipeline3_round1)
    predictions = best_estimator_pipeline3_round1.predict(X_final)
    return predictions

# predicted values for Test data set
y_test_pred = final_pipeline(X_test)

"""### <font color ='pickle'>**Classification report for test dataset**"""

print('\nTest set classification report:\n\n',
      classification_report(y_test, y_test_pred))

"""## <font color ='pickle'>**Another Approach - Stacking**

<font color ='indianred'>**Stacking: Try different models on manual features and different models using tfidf. Use predictions of these modes as new features.**
"""